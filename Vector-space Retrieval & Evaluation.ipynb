{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-space Retrieval & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Step 1: Load Inverted Index (H) and compute DocLen (DL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # terms: 11463\n",
      " - Entry for 'pentobarbit': df=4, idf=2.412040330191658\n",
      " - Entry for 'defici': df=39, idf=1.4230357144931214\n",
      " - Entry for 'treatment': df=172, idf=0.7785718746120717\n",
      "\n",
      "Total # documents: 1033\n",
      " - Vector len for Doc 59 = 13.811725366348801\n",
      " - Vector len for Doc 1033 = 31.163653356034512\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "tindexfile = 'medline_term_index.csv'\n",
    "invindexfile = 'medline_inverted_index.csv'\n",
    "dindexfile = 'medline_doc_index.csv'\n",
    "\n",
    "# Number of documents in the corpus (hard-coded for this corpus)\n",
    "N = 1033\n",
    "\n",
    "# Major data structures\n",
    "H_invindex = {} # inverted index; term -> (idf, L:hashmap of (docID . tf))\n",
    "DL_doclen = {}  # document lengths; docID -> len\n",
    "\n",
    "## (1) Read the term index file and populate the invindex first\n",
    "tid2term_map = {} # temporary storage to hold mappings of termID -> term\n",
    "\n",
    "fin = open(tindexfile, 'r', encoding='utf-8')\n",
    "reader = csv.reader(fin, delimiter='\\t')\n",
    "for line in reader:\n",
    "    term = line[0]    # term string\n",
    "    termID = line[1]  # termID\n",
    "    df = int(line[2]) # document frequency\n",
    "    idf = math.log10(N/df) # idf\n",
    "    # record term -> (idf, emptyL) in H\n",
    "    H_invindex[term] = (idf, dict())\n",
    "    # record termID -> term \n",
    "    tid2term_map[termID] = term \n",
    "fin.close()\n",
    "\n",
    "## (2) Read the inverted index file and add postings lists in H.\n",
    "## Also compute document lengths too, incrementally -- and record in DL.\n",
    "fin = open(invindexfile, 'r')\n",
    "reader = csv.reader(fin, delimiter='\\t')\n",
    "for line in reader:\n",
    "    termID = line[0]\n",
    "    idx = 1\n",
    "    while idx < (len(line)-1):\n",
    "        docID = line[idx]\n",
    "        tf = int(line[idx+1]) # raw tf of the term in this document\n",
    "        # Record docID -> tf in term's L\n",
    "        L = (H_invindex[tid2term_map[termID]])[1]\n",
    "        L[docID] = tf  # docID -> raw term frequency\n",
    "        \n",
    "        # Accumulate the component vector length for the document\n",
    "        tfidf = tf * (H_invindex[tid2term_map[termID]])[0] # tf * idf\n",
    "        tfidfsq = math.pow(tfidf, 2.0)\n",
    "        if docID in DL_doclen:\n",
    "            DL_doclen[docID] += tfidfsq\n",
    "        else:\n",
    "            DL_doclen[docID] = tfidfsq\n",
    "        #\n",
    "        idx += 2\n",
    "fin.close()\n",
    "\n",
    "# Fix the DL entries by applying sqrt to make vector length.\n",
    "for docID in DL_doclen.keys():\n",
    "    val = DL_doclen[docID]\n",
    "    DL_doclen[docID] = math.sqrt(val)\n",
    "\n",
    "    \n",
    "print ('Total # terms: %d' % len(H_invindex))\n",
    "for term in ['pentobarbit', 'defici', 'treatment']:\n",
    "    print (' - Entry for \\'%s\\': df=%s, idf=%s' % (term, len(H_invindex[term][1]), H_invindex[term][0]))\n",
    "\n",
    "print ('\\nTotal # documents: %d' % len(DL_doclen))\n",
    "for docID in ['59', '1033']:\n",
    "    print (' - Vector len for Doc %s = %s' % (docID, DL_doclen[docID]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Step 2: Queries as Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # queries: 30\n",
      " - Query 2: {'relationship': 1, 'blood': 1, 'cerebrospin': 1, 'fluid': 1, 'oxygen': 1, 'concentr': 1, 'partial': 1, 'pressur': 1, 'method': 1, 'interest': 1, 'polarographi': 1}\n",
      " - Query 22: {'mycoplasma': 1, 'infect': 1, 'presenc': 1, 'embryo': 1, 'fetu': 1, 'newborn': 1, 'infant': 1, 'anim': 1, 'pregnanc': 1, 'gynecolog': 1, 'diseas': 1, 'relat': 1, 'chromosom': 2, 'abnorm': 1}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "queryfile = 'medline.query'\n",
    "\n",
    "# A list of queries. Each Query is a tuple, (qID, Q:term->tf map)\n",
    "Queries_list = []\n",
    "\n",
    "fin = open(queryfile, 'r', encoding='utf-8')#'iso-8859-1')\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "for line in fin:\n",
    "    matchObj = re.match(r'^(\\d+)\\s+(.*)', line)\n",
    "    if not matchObj:\n",
    "        print (\"ERROR with line -- %s\" % line)\n",
    "    else:\n",
    "        queryID = matchObj.group(1) # queryID\n",
    "        text = matchObj.group(2)    # query string (ignoring sentences)\n",
    "\n",
    "        # process text string -- same processing as one applied to documents.\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        terms = [porter.stem(w) for w in tokens if w not in stopwords.words('english') and len(w) > 1] # (**)\n",
    "        # term frequencies of the terms in this query are obtained by NLTK's FreqDist\n",
    "        fdist = nltk.FreqDist(terms)\n",
    "        # append the Query in the list\n",
    "        Queries_list.append((queryID, dict(fdist)))\n",
    "fin.close()\n",
    "\n",
    "print ('Total # queries: %d' % len(Queries_list))\n",
    "for qid in [1, 21]:\n",
    "    print (' - Query %s: %s' % (Queries_list[qid][0], Queries_list[qid][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Retrieval and Ranking \n",
    "\n",
    "### - For each query, apply the Vector Space Retrieval Algorithm and obtain a ranked list of retrieved documents (sorted by the cosine measure) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlst=[]\n",
    "\n",
    "for c in range(len(Queries_list)):\n",
    "    hash_map_score={}\n",
    "    q_length=[]\n",
    "    \n",
    "    for i in Queries_list[c][1]:\n",
    "        \n",
    "        doc_collect=[]\n",
    "        try:\n",
    "            idf=H_invindex[i][0]\n",
    "        except:\n",
    "            continue\n",
    "        for d,f in H_invindex[i][1].items():\n",
    "            doc_collect.append((d,f*idf*Queries_list[c][1][i]*idf)) #tfidf Dt *tfidf Qt\n",
    "\n",
    "        for d_score in doc_collect:\n",
    "            if d_score[0] in hash_map_score.keys():\n",
    "                hash_map_score[d_score[0]]+=d_score[1]\n",
    "            else:\n",
    "                hash_map_score[d_score[0]]=d_score[1]\n",
    "        \n",
    "        q_length.append(Queries_list[c][1][i]*idf)\n",
    "        \n",
    "    q_length1=np.sqrt(np.dot(q_length,q_length))\n",
    "    qlst.append([Queries_list[c][0],hash_map_score,q_length1])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_score={}\n",
    "for i in range(len(qlst)):\n",
    "    d_collection=[]\n",
    "    for (k,v) in qlst[i][1].items():\n",
    "        d_collection.append((k,v/(DL_doclen[k]*qlst[i][2])))\n",
    "        \n",
    "    tb=sorted(d_collection, key=lambda tup: tup[1],reverse=True)\n",
    "    cosine_score[qlst[i][0]]=tb\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Evaluation -- Compute MAP\n",
    "### - Compare the ranked lists with the anwers, and compute the MAP score.\n",
    "\n",
    "For each query:\n",
    "1.\tI first sort the relevant documents based on the document ranking order from the cosine score computed in the previous part (Part 3 Retrieval and Ranking)\n",
    "2.\tthen I compute the precision score for the relevant document and append it to an empty list\n",
    "3.\tlastly, in the end of the loop I will compute the average precision score by summing up all the value in the list where I stored the precision values for the relevant document and divided it by the length of that list.\n",
    "4.\tThen I append the average precision score for each query to another list that is used to store the average precision score for each list.\n",
    "5.After each query is loop through, to compute the mean average precision score, I summed up all the average precision score for each query and divided it by the length of that list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import file\n",
    "fin = open('medline.rel', 'r', encoding='utf-8')\n",
    "ls=[]\n",
    "for line in fin:\n",
    "    ls.append(re.split(\"\\n\",line))\n",
    "    \n",
    "# relevancy answers formating\n",
    "lst={}\n",
    "for i in range(len(ls)):\n",
    "    lst[ls[i][0].split()[0]]=ls[i][0].split()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_lst=[]\n",
    "\n",
    "for i in cosine_score.keys():\n",
    "    doc_count={}\n",
    "    for d_key in return_key(cosine_score[i]):\n",
    "        doc_count[d_key]=0\n",
    "        \n",
    "    for relavant in lst[i]:\n",
    "        if relavant in doc_count.keys():\n",
    "            doc_count[relavant]+=1  \n",
    "    \n",
    "    query_lst.append((i,[c for c in enumerate(doc_count.items(),1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precision=[]\n",
    "\n",
    "for i in range(len(query_lst)):\n",
    "    numerator=0\n",
    "    precision_collect=[]\n",
    "    \n",
    "    for a in range(len(query_lst[i][1])):\n",
    "        if query_lst[i][1][a][1][1]!= 0:\n",
    "            numerator+=1\n",
    "            precision_collect.append(numerator/query_lst[i][1][a][0])\n",
    "            \n",
    "    avg_precision.append(sum(precision_collect)/len(precision_collect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5656605218653455"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the final MAP \n",
    "sum(avg_precision)/len(avg_precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
